# üé¨ Sniffer - Video Processing and Transcription Tool

A comprehensive Python tool for processing video files to extract audio, frames, and generate transcriptions with precise word-level timestamps using OpenAI's Whisper API.

## ‚ú® Features

- **üéµ Audio Extraction**: Extract high-quality audio from MP4 videos
- **üñºÔ∏è Frame Extraction**:
  - Extract all frames with millisecond timestamps
  - Extract frames at specific positions per second (start, middle, end, random)
- **üéôÔ∏è AI Transcription**: Generate transcriptions with word-level timestamps using OpenAI Whisper
- **‚è±Ô∏è Second-Aligned Words**: Map words to exact seconds for perfect frame-transcript synchronization
- **üìÅ Batch Processing**: Process single videos or entire directories
- **üîÑ Advanced Frame-Audio Sync**: Align transcript words with extracted frames with speech coverage analysis
- **üé® Beautiful CLI**: Rich terminal interface with progress tracking and tables
- **üß™ Comprehensive Testing**: Full test suite with pytest
- **üìä Structured Logging**: Professional logging system

## üöÄ Quick Start

### System Requirements

SunDogs requires **FFmpeg** for video processing. Install it first:

**macOS (with Homebrew):**
```bash
brew install ffmpeg
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install ffmpeg
```

**Windows:**
- Download from [FFmpeg official website](https://ffmpeg.org/download.html)
- Or use [Chocolatey](https://chocolatey.org/): `choco install ffmpeg`

**Verify installation:**
```bash
ffmpeg -version
```

### Installation

```bash
cd sniffer

# Install with uv (recommended)
uv sync
```

### Environment Setup

Create a `.env` file in the project root for configuration:

```bash
# Copy the example file
cp .env.example .env

# Edit the .env file with your settings
# At minimum, set your OpenAI API key for transcription features
OPENAI_API_KEY=your-api-key-here
```

The `.env` file supports the following configuration options:

**Required:**
- `OPENAI_API_KEY` - Your OpenAI API key for transcription features

**Optional (Logging):**
- `LOG_LEVEL` - Logging level (DEBUG, INFO, WARNING, ERROR) [default: INFO]
- `LOG_FILE` - Path to log file [default: logs/sniffer.log]
- `LOG_ROTATION` - Log rotation setting [default: 10 MB]
- `LOG_RETENTION` - Log retention period [default: 10 days]
- `LOG_COLORIZE` - Enable colored console output [default: true]

### Basic Usage

```bash
# Setup directories and check dependencies
sniffer setup

# Process a single video with transcription
sniffer process video.mp4 --frames middle --transcribe

# Batch process a folder of videos
sniffer process ./videos --frames random

# Get information about videos without processing
sniffer info ./videos
```

## üìñ CLI Commands

### `process` - Main Processing Command

Process video files to extract audio, frames, and generate transcriptions.

```bash
sniffer process INPUT_PATH [OPTIONS]
```

**Options:**
- `--audio/--no-audio`: Extract audio (default: enabled)
- `--frames [start|middle|end|random]`: Extract frames by position per second
- `--all-frames`: Extract ALL frames (‚ö†Ô∏è large output!)
- `--transcribe`: Transcribe audio with word-level timestamps
- `--verbose, -v`: Verbose output with detailed configuration

**Examples:**
```bash
# Process with middle frames and transcription
sniffer process video.mp4 --frames middle --transcribe

# Batch process folder with audio only
sniffer process ./videos

# Extract all frames without audio
sniffer process video.mp4 --all-frames --no-audio
```

### `info` - Video Information

Show information about video files without processing them.

```bash
sniffer info INPUT_PATH
```

### `setup` - Environment Setup

Setup required directories and check dependencies.

```bash
sniffer setup
```

## üêç Python API

Use Sniffer programmatically in your Python applications:

### Core Video Processing

```python
from sniffer import VideoProcessor, AudioTranscriber
from pathlib import Path

# Initialize processor for single video
processor = VideoProcessor("path/to/video.mp4")

# Extract audio and frames
audio_path = processor.extract_audio()
frames = processor.extract_frames_by_position("middle")
metadata = processor.get_video_metadata()

# Complete processing pipeline
results = processor.process_all(
    extract_audio=True,
    extract_all_frames=False,
    frame_position="middle"
)
```

### üÜï Second-Aligned Transcription

Perfect synchronization between frames and transcript words:

```python
from sniffer import AudioTranscriber

# Transcribe with enhanced word-level timestamps
transcriber = AudioTranscriber("audio.mp3")
transcript = transcriber.transcribe()

# üéØ NEW: Get words with second mapping
enhanced_words = transcriber.extract_word_timestamps(transcript)
# Each word includes: word, start, end, seconds_spoken
# Example: {"word": "hello", "start": 0.0, "end": 0.5, "seconds_spoken": [0]}

# üéØ NEW: Get words organized by second
words_by_second = transcriber.extract_words_by_second(transcript)
# Returns: {0: [words in second 0], 1: [words in second 1], ...}

# üéØ NEW: Quick lookup for specific second
words_at_second_5 = transcriber.get_words_for_second(transcript, 5)
# Returns all words spoken during second 5

# üéØ NEW: Advanced frame-transcript synchronization
frame_seconds = [0, 1, 2, 3, 4, 5]  # Seconds where you extracted frames
sync_data = transcriber.synchronize_transcript_with_frames(transcript, frame_seconds)

# Analyze each frame's speech content
for second, data in sync_data.items():
    if data["has_speech"]:
        primary_words = ", ".join(data["primary_words"])
        coverage = data["speech_coverage"] * 100
        print(f"Frame {second}s: '{primary_words}' ({coverage:.0f}% speech)")
    else:
        print(f"Frame {second}s: (silence)")
```

### Service Layer Usage (Advanced)

For fine-grained control over processing:

```python
from sniffer.services import VideoMetadataService, FrameExtractionService
from sniffer.services.frame_extraction import FrameExtractionConfig
from pathlib import Path

# Direct service usage
video_path = Path("video.mp4")

# Extract metadata with dedicated service
metadata_service = VideoMetadataService()
metadata = metadata_service.extract_metadata(video_path)

# Configure frame extraction precisely
frame_service = FrameExtractionService()
config = FrameExtractionConfig(
    video_path=video_path,
    position="random",
    output_dir="custom/frames/"
)
frames = frame_service.extract_frames_by_position(config)
```

### Complete Workflow Example

```python
from pathlib import Path
from sniffer import VideoProcessor, AudioTranscriber

# 1. Process video
video_file = Path("presentation.mp4")
processor = VideoProcessor(video_file)
results = processor.process_all(
    extract_audio=True,
    frame_position="middle"  # Extract middle frame of each second
)

# 2. Transcribe with second-aligned mapping
if results.get("audio_path"):
    transcriber = AudioTranscriber(results["audio_path"])
    transcript = transcriber.transcribe()

    # 3. Get frame seconds from processing results
    frame_seconds = list(results.get("position_frames", {}).keys())

    # 4. Synchronize transcript with extracted frames
    sync_data = transcriber.synchronize_transcript_with_frames(
        transcript, frame_seconds
    )

    # 5. Analyze synchronized content
    for second in frame_seconds:
        frame_path = results["position_frames"][second]
        words_data = sync_data[second]

        if words_data["has_speech"]:
            words = ", ".join(words_data["primary_words"])
            print(f"üì∏ Frame: {Path(frame_path).name}")
            print(f"üí¨ Speech: '{words}' ({words_data['word_count']} words)")
            print(f"üìä Coverage: {words_data['speech_coverage']*100:.0f}%")
        else:
            print(f"üì∏ Frame: {Path(frame_path).name} (visual only)")
        print()
```

## üó∫Ô∏è Architecture & Call Flow

### System Overview

```mermaid
graph TB
    CLI[CLI Entry Point<br/>main.py] --> |setup| SETUP[setup_directories]
    CLI --> |process| PROCESS[ProcessHandler]
    CLI --> |info| INFO[DisplayManager]

    PROCESS --> VP[VideoProcessor]
    PROCESS --> AT[AudioTranscriber]

    VP --> |services| MS[VideoMetadataService]
    VP --> |services| FS[FrameExtractionService]
    VP --> |extract_audio| AUDIO[Audio Extraction]

    FS --> |context manager| VC[VideoCapture]
    MS --> VC
    VC --> |OpenCV| CV[cv2.VideoCapture]

    AUDIO --> |MoviePy| MP3[MP3 Files]
    FS --> |OpenCV| PNG[PNG Files]

    AT --> |transcribe| WHISPER[OpenAI Whisper API]
    WHISPER --> JSON[JSON Transcripts]

    VP --> UTILS[Utils Layer]
    AT --> UTILS
    MS --> UTILS
    FS --> UTILS

    UTILS --> FILE[File Operations]
    UTILS --> DIR[Directory Management]
    UTILS --> LOG[Logging System]
```

### Core Component Flow

#### 1. CLI Entry Points (`main.py`)

```python
# Command flow
app.command("process") ‚Üí process() ‚Üí {
    ProcessHandler() ‚Üí {
        VideoProcessor(input_path) ‚Üí {
            VideoMetadataService() ‚Üí metadata extraction
            FrameExtractionService() ‚Üí frame processing
            ‚îú‚îÄ‚îÄ extract_audio() ‚Üí list[str]
            ‚îú‚îÄ‚îÄ extract_frames_by_position() ‚Üí dict[int, str]
            ‚îî‚îÄ‚îÄ extract_all_frames() ‚Üí list[str]
        }

        AudioTranscriber() ‚Üí {
            ‚îú‚îÄ‚îÄ transcribe() ‚Üí dict
            ‚îî‚îÄ‚îÄ synchronize_with_frames() ‚Üí list[dict]
        }
    }

    DisplayManager() ‚Üí {
        ‚îú‚îÄ‚îÄ show_processing_config()
        ‚îú‚îÄ‚îÄ show_results_summary()
        ‚îî‚îÄ‚îÄ show_video_info_table()
    }
}
```

#### 2. VideoProcessor Class (`video_processor.py`)

```python
VideoProcessor(video_file) ‚Üí {
    __init__() ‚Üí {
        ‚îú‚îÄ‚îÄ VideoMetadataService() ‚Üí metadata operations
        ‚îú‚îÄ‚îÄ FrameExtractionService() ‚Üí frame operations
        ‚îú‚îÄ‚îÄ ensure_directory() ‚Üí utils.directory
        ‚îî‚îÄ‚îÄ get_logger() ‚Üí utils.logging
    }

    # Public Methods
    ‚îú‚îÄ‚îÄ extract_audio() ‚Üí _extract_single_audio() ‚Üí MoviePy
    ‚îú‚îÄ‚îÄ extract_all_frames() ‚Üí FrameExtractionService.extract_all_frames()
    ‚îú‚îÄ‚îÄ extract_frames_by_position() ‚Üí FrameExtractionService.extract_frames_by_position()
    ‚îú‚îÄ‚îÄ get_video_metadata() ‚Üí VideoMetadataService.extract_metadata()
    ‚îî‚îÄ‚îÄ process_all() ‚Üí Orchestrates all operations
}
```

#### 3. AudioTranscriber Class (`transcription.py`)

```python
AudioTranscriber(api_key) ‚Üí {
    __init__() ‚Üí OpenAI(api_key)

    # Core Methods
    ‚îú‚îÄ‚îÄ transcribe_with_timestamps() ‚Üí OpenAI.audio.transcriptions.create()
    ‚îú‚îÄ‚îÄ transcribe_batch() ‚Üí {
    ‚îÇ   ‚îú‚îÄ‚îÄ transcribe_with_timestamps() (per file)
    ‚îÇ   ‚îî‚îÄ‚îÄ save_transcripts ‚Üí JSON files
    ‚îÇ   }

    # Analysis Methods
    ‚îú‚îÄ‚îÄ extract_word_timestamps() ‚Üí list[dict]
    ‚îú‚îÄ‚îÄ extract_segments() ‚Üí list[dict]
    ‚îú‚îÄ‚îÄ get_text_at_timestamp() ‚Üí str | None
    ‚îî‚îÄ‚îÄ synchronize_with_frames() ‚Üí list[dict]
}
```

#### 4. Service Layer

```python
services/ ‚Üí {
    video_metadata.py ‚Üí {
        VideoMetadataService() ‚Üí {
            ‚îú‚îÄ‚îÄ extract_metadata() ‚Üí VideoMetadata | dict
            ‚îú‚îÄ‚îÄ _extract_opencv_metadata() ‚Üí dict
            ‚îú‚îÄ‚îÄ _extract_moviepy_metadata() ‚Üí dict
            ‚îî‚îÄ‚îÄ get_basic_info() ‚Üí tuple[float, int, float]
        }
    }

    frame_extraction.py ‚Üí {
        FrameExtractionService() ‚Üí {
            ‚îú‚îÄ‚îÄ extract_all_frames() ‚Üí list[str]
            ‚îú‚îÄ‚îÄ extract_frames_by_position() ‚Üí dict[int, str]
            ‚îú‚îÄ‚îÄ _get_video_info() ‚Üí tuple[float, int, float]
            ‚îú‚îÄ‚îÄ _calculate_timestamps_per_second() ‚Üí list[tuple[int, int]]
            ‚îî‚îÄ‚îÄ _fetch_frames_by_timestamp() ‚Üí dict[int, str]
        }

        FrameExtractionConfig() ‚Üí {
            ‚îú‚îÄ‚îÄ video_path: Path
            ‚îú‚îÄ‚îÄ position: Optional[str]
            ‚îú‚îÄ‚îÄ extract_all: bool
            ‚îî‚îÄ‚îÄ output_dir: Optional[str]
        }
    }

    video_capture.py ‚Üí {
        VideoCapture(video_path) ‚Üí {
            ‚îú‚îÄ‚îÄ __enter__() ‚Üí cv2.VideoCapture
            ‚îú‚îÄ‚îÄ __exit__() ‚Üí resource cleanup
            ‚îî‚îÄ‚îÄ is_opened ‚Üí bool property
        }
    }
}
```

#### 5. CLI Layer

```python
cli/ ‚Üí {
    process_handler.py ‚Üí {
        ProcessHandler() ‚Üí {
            ‚îú‚îÄ‚îÄ process_videos() ‚Üí tuple[list[ProcessResults], dict]
            ‚îú‚îÄ‚îÄ _process_video_files() ‚Üí list[ProcessResults]
            ‚îî‚îÄ‚îÄ _process_transcriptions() ‚Üí dict
        }
    }

    display.py ‚Üí {
        DisplayManager() ‚Üí {
            ‚îú‚îÄ‚îÄ show_processing_config() ‚Üí None
            ‚îú‚îÄ‚îÄ show_results_summary() ‚Üí None
            ‚îú‚îÄ‚îÄ show_video_info_table() ‚Üí None
            ‚îú‚îÄ‚îÄ show_setup_status() ‚Üí None
            ‚îú‚îÄ‚îÄ print() ‚Üí console.print wrapper
            ‚îî‚îÄ‚îÄ print_exception() ‚Üí console.print_exception wrapper
        }
    }
}
```

#### 6. Utils Layer

```python
utils/ ‚Üí {
    file.py ‚Üí {
        ‚îú‚îÄ‚îÄ extract_filename_from_path()
        ‚îú‚îÄ‚îÄ is_video_file() / is_audio_file()
        ‚îú‚îÄ‚îÄ get_file_size() / format_file_size()
        ‚îî‚îÄ‚îÄ ensure_file_exists()
    }

    directory.py ‚Üí {
        ‚îú‚îÄ‚îÄ ensure_directory() / ensure_directories()
        ‚îú‚îÄ‚îÄ is_directory_empty()
        ‚îú‚îÄ‚îÄ list_files_in_directory()
        ‚îî‚îÄ‚îÄ clean_directory()
    }

    logging.py ‚Üí {
        ‚îú‚îÄ‚îÄ setup_default_logging()
        ‚îú‚îÄ‚îÄ get_logger() ‚Üí Loguru instance
        ‚îî‚îÄ‚îÄ ProgressLogger ‚Üí {
            ‚îú‚îÄ‚îÄ start_operation()
            ‚îú‚îÄ‚îÄ progress_update()
            ‚îú‚îÄ‚îÄ complete_operation()
            ‚îî‚îÄ‚îÄ operation_error()
        }
    }
}
```

### Data Flow Examples

#### Complete Processing Pipeline

```python
# CLI Command: uv run sniffer process video.mp4 --frames middle --transcribe

main.process() ‚Üí {
    1. VideoProcessor("video.mp4") ‚Üí {
        ‚îú‚îÄ‚îÄ _get_video_files() ‚Üí [Path("video.mp4")]
        ‚îî‚îÄ‚îÄ setup directories
    }

    2. processor.process_all() ‚Üí {
        ‚îú‚îÄ‚îÄ extract_audio() ‚Üí ["data/audio/video.mp3"]
        ‚îî‚îÄ‚îÄ extract_frames_by_position("middle") ‚Üí {
            "video.mp4": {0: "frame_s0_500ms.png", 1: "frame_s1_1500ms.png"}
        }
    }

    3. AudioTranscriber() ‚Üí {
        ‚îú‚îÄ‚îÄ transcribe_batch(["data/audio/video.mp3"]) ‚Üí {
        ‚îÇ   "video.mp3": {
        ‚îÇ       "text": "transcript...",
        ‚îÇ       "words": [{"word": "hello", "start": 0.0, "end": 0.5}]
        ‚îÇ   }
        ‚îÇ   }
        ‚îî‚îÄ‚îÄ save transcripts ‚Üí "data/transcripts/video_transcript.json"
    }

    4. show_results_summary() ‚Üí Rich table display
}
```

#### Batch Processing Flow

```python
# CLI Command: uv run sniffer process ./videos --frames random

VideoProcessor("./videos") ‚Üí {
    _get_video_files() ‚Üí [
        Path("videos/video1.mp4"),
        Path("videos/video2.mp4"),
        Path("videos/video3.mp4")
    ]

    process_all() ‚Üí {
        # Parallel processing for each video
        for video_file in video_files:
            ‚îú‚îÄ‚îÄ _extract_single_audio(video_file)
            ‚îî‚îÄ‚îÄ _extract_frames_by_position_single(video_file, "random")
    }
}
```

### External Dependencies Integration

```python
# System Integration Points
{
    "FFmpeg": "Required by MoviePy for video processing",
    "OpenCV": "Direct integration for frame extraction",
    "OpenAI API": "Whisper model for transcription",
    "File System": "utils.directory & utils.file for I/O operations"
}
```

## üìÅ Output Structure

SunDogs organizes output files in a clean directory structure:

```
data/
‚îú‚îÄ‚îÄ audio/                  # Extracted audio files (.mp3)
‚îú‚îÄ‚îÄ video_frames/          # Extracted frames (.png)
‚îÇ   ‚îî‚îÄ‚îÄ video_name/        # Frames grouped by video
‚îú‚îÄ‚îÄ transcripts/           # Transcription files (.json)
‚îî‚îÄ‚îÄ video/                 # Original video files
```

### Frame Naming Convention

- **All frames**: `frame_ts_00001234.png` (timestamp in milliseconds)
- **Position frames**: `frame_s2_1500ms.png` (second 2, at 1500ms)

### Transcript Format

**Original OpenAI Whisper Format:**
```json
{
  "text": "Full transcript text",
  "words": [
    {
      "word": "Hello",
      "start": 0.0,
      "end": 0.5
    }
  ],
  "segments": [
    {
      "text": "Hello world",
      "start": 0.0,
      "end": 1.0,
      "words": [...]
    }
  ]
}
```

**üÜï Enhanced Second-Aligned Format:**

*Words with second mapping:*
```json
{
  "word": "Hello",
  "start": 0.0,
  "end": 0.5,
  "seconds_spoken": [0]
}
```

*Words organized by second:*
```json
{
  "0": [
    {"word": "Hello", "start": 0.0, "end": 0.5, "duration_in_second": 0.5}
  ],
  "1": [
    {"word": "world", "start": 0.6, "end": 1.2, "duration_in_second": 0.4}
  ]
}
```

*Frame synchronization data:*
```json
{
  "0": {
    "second": 0,
    "words": [...],
    "word_count": 2,
    "speech_coverage": 0.9,
    "primary_words": ["Hello", "world"],
    "has_speech": true
  }
}
```

## ‚öôÔ∏è Configuration

### Environment Variables

- `OPENAI_API_KEY`: Your OpenAI API key for transcription features

### Video Support

Currently supports **MP4 files only**. Additional formats can be added by extending the `VideoProcessor` class.

### Frame Positions

- **start**: First frame of each second
- **middle**: Middle frame of each second
- **end**: Last frame of each second
- **random**: Random frame within each second

## üß™ Development

### Running Tests

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=sniffer

# Run specific test file
uv run pytest tests/test_video_processor.py -v
```

### Code Quality

```bash
# Format code
uv run ruff format .

# Lint code
uv run ruff check .

# Type checking
uv run mypy sniffer/

# Full CI pipeline
make ci
```

### Project Structure

```
sniffer/
‚îú‚îÄ‚îÄ __init__.py                 # Package exports
‚îú‚îÄ‚îÄ main.py                     # CLI entry point
‚îú‚îÄ‚îÄ video_processor.py          # Video processing orchestrator
‚îú‚îÄ‚îÄ transcription.py            # Audio transcription
‚îú‚îÄ‚îÄ cli/                        # CLI layer
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # CLI package exports
‚îÇ   ‚îú‚îÄ‚îÄ process_handler.py     # Video processing workflow
‚îÇ   ‚îî‚îÄ‚îÄ display.py             # Rich console output management
‚îú‚îÄ‚îÄ services/                   # Service layer
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # Services package exports
‚îÇ   ‚îú‚îÄ‚îÄ video_metadata.py      # Video metadata extraction
‚îÇ   ‚îú‚îÄ‚îÄ frame_extraction.py    # Frame extraction operations
‚îÇ   ‚îî‚îÄ‚îÄ video_capture.py       # Resource-managed video capture
‚îú‚îÄ‚îÄ config/                     # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # Config package exports
‚îÇ   ‚îî‚îÄ‚îÄ constants.py           # Application constants
‚îî‚îÄ‚îÄ utils/                      # Utility functions
    ‚îú‚îÄ‚îÄ __init__.py            # Utils package exports
    ‚îú‚îÄ‚îÄ file.py                # File operations
    ‚îú‚îÄ‚îÄ directory.py           # Directory management
    ‚îî‚îÄ‚îÄ logging.py             # Logging system
```

## üîß Requirements

### System Dependencies
- **FFmpeg**: Required for video processing (see installation instructions above)

### Python Dependencies
- **Python**: 3.13+ (uses modern syntax features)
- **OpenCV**: Video and image processing
- **MoviePy**: Video file manipulation (requires FFmpeg)
- **OpenAI**: Whisper API for transcription
- **Loguru**: Modern logging library
- **python-dotenv**: Environment variable management
- **Typer + Rich**: Beautiful CLI interface

## üöÄ Real-World Use Cases

### üìö Educational Content Analysis

Perfect for analyzing educational videos, lectures, and tutorials:

```python
from sniffer import VideoProcessor, AudioTranscriber

# Process educational video
processor = VideoProcessor("lecture.mp4")
results = processor.process_all(extract_audio=True, frame_position="middle")

# Get second-by-second analysis
transcriber = AudioTranscriber(results["audio_path"])
transcript = transcriber.transcribe()
frame_seconds = list(results["position_frames"].keys())

# Sync transcript with visual content
sync_data = transcriber.synchronize_transcript_with_frames(transcript, frame_seconds)

# Identify key educational moments
key_moments = []
for second, data in sync_data.items():
    if data["speech_coverage"] > 0.7:  # High speech activity
        keywords = data["primary_words"]
        if any(word in ["important", "key", "remember", "note"] for word in keywords):
            key_moments.append({
                "second": second,
                "frame": results["position_frames"][second],
                "keywords": keywords,
                "importance": "high"
            })

print(f"Found {len(key_moments)} key educational moments!")
```

### üé¨ Video Content Indexing

Create searchable video indexes with frame-accurate word positioning:

```python
# Build searchable index
def create_video_index(video_path):
    processor = VideoProcessor(video_path)
    results = processor.process_all(extract_audio=True, frame_position="start")

    transcriber = AudioTranscriber(results["audio_path"])
    transcript = transcriber.transcribe()

    # Create searchable word index with exact frame references
    word_index = {}
    words_by_second = transcriber.extract_words_by_second(transcript)

    for second, words in words_by_second.items():
        for word_data in words:
            word = word_data["word"].lower()
            if word not in word_index:
                word_index[word] = []

            word_index[word].append({
                "second": second,
                "frame_path": results["position_frames"].get(second),
                "confidence": word_data["duration_in_second"],
                "context": [w["word"] for w in words]
            })

    return word_index

# Usage
index = create_video_index("presentation.mp4")
search_results = index.get("algorithm", [])  # Find all mentions of "algorithm"
```

### üé≠ Content Moderation & Analysis

Automatically detect and flag content based on speech-visual correlation:

```python
def analyze_content_safety(video_path, flagged_terms):
    processor = VideoProcessor(video_path)
    results = processor.process_all(extract_audio=True, frame_position="random")

    transcriber = AudioTranscriber(results["audio_path"])
    transcript = transcriber.transcribe()

    # Check each second for flagged content
    alerts = []
    for second in range(int(transcript.get("words", [])[-1].get("end", 0)) + 1):
        words_data = transcriber.get_words_for_second(transcript, second)
        spoken_words = [w["word"].lower() for w in words_data]

        # Check for flagged terms
        for term in flagged_terms:
            if term.lower() in " ".join(spoken_words):
                alerts.append({
                    "second": second,
                    "term": term,
                    "context": spoken_words,
                    "frame": results["position_frames"].get(second),
                    "severity": "high" if any("explicit" in w for w in spoken_words) else "medium"
                })

    return alerts
```

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üêõ Troubleshooting

### Common Issues

**"No module named 'ffmpeg'" or video processing errors**
- Install FFmpeg system-wide (see System Requirements section)
- Verify FFmpeg is in your PATH: `ffmpeg -version`
- Restart your terminal after installation

**"No MP4 files found"**
- Ensure your video files have the `.mp4` extension
- Check that the directory path is correct

**"OpenAI API key is required"**
- Create a `.env` file and set `OPENAI_API_KEY=your-key-here`
- Or set the environment variable: `export OPENAI_API_KEY='your-key'`
- Verify your API key is valid and has credits

**"Could not open video file"**
- Verify the video file is not corrupted
- Ensure FFmpeg is properly installed
- Check that OpenCV can read the video format

### Getting Help

- Check the [Issues](https://github.com/erictt/sniffer/issues) page
- Create a new issue with detailed information about your problem
- Include log files when reporting bugs (use `--verbose` flag)

---

Made with ‚ù§Ô∏è for video processing and AI transcription
